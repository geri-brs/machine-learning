{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kanye_network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTZb3Llop6C_"
      },
      "source": [
        "# https://goo.gl/GHSWzZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f7c1rJ8FhDb"
      },
      "source": [
        "1. Run this chunk of code first - it gets your dataset ready."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haowk4QumdIy",
        "cellView": "code",
        "outputId": "58af79f7-54d7-42db-fcbd-4858f912952b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Dataset\n",
        "import urllib2\n",
        "# kanye lyrics: https://pastebin.com/raw/9S5u08EU\n",
        "\n",
        "target_url = \"https://pastebin.com/raw/9S5u08EU\" # go to pastebin, paste in your rap lyrics, submit that and then click \"raw\", or swap it out for one of the URL's above\n",
        "dirty_rap_source = urllib2.urlopen(target_url).read()\n",
        "rap_source = [x.split(\"\\r\")[0] for x in dirty_rap_source.split(\"\\n\")]\n",
        "while \"\" in rap_source:\n",
        "  rap_source.remove(\"\")\n",
        "while \" \" in rap_source:\n",
        "  rap_source.remove(\" \")\n",
        "print \"Your dataset's first 3 rap lines: \"\n",
        "print rap_source[:3]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your dataset's first 3 rap lines: \n",
            "['Let the suicide doors up', 'I threw suicides on the tour bus', 'I threw suicides on the private jet']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q11ztMRFswC"
      },
      "source": [
        "2. Run this chunk of code next - it installs all of the dependencies that you need to run the rap network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJqTBepLhs_b",
        "outputId": "3a464240-46f1-4bdd-ad89-458637ad93fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "# Dependencies\n",
        "# just installing different packages that are needed.\n",
        "# markovify handles markov chains\n",
        "# pronouncing handles rhymes\n",
        "!pip install markovify\n",
        "!pip install pronouncing"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting markovify\n",
            "  Downloading https://files.pythonhosted.org/packages/33/92/4036691c7ea53e545e98e0ffffcef357ca19aa2405df366ae5b8b7da391a/markovify-0.8.3.tar.gz\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 8.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.8.3-cp27-none-any.whl size=18415 sha256=5c0034c23057413a655752d837cee66c25b28df315112266c484ff930bec32b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/e5/be/8e61715070048813947af5fb32f47b4cf9dddd37c965800bdb\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.8.3 unidecode-1.1.1\n",
            "Collecting pronouncing\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/c6/9dc74a3ddca71c492e224116b6654592bfe5717b4a78582e4d9c3345d153/pronouncing-0.2.0.tar.gz\n",
            "Collecting cmudict>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/77/f009abf803286876fa99cb7bd9d1132c7b64a0b34a0360666275ce1bc733/cmudict-0.4.5-py2.py3-none-any.whl (939kB)\n",
            "\u001b[K     |████████████████████████████████| 942kB 7.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pronouncing\n",
            "  Building wheel for pronouncing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pronouncing: filename=pronouncing-0.2.0-py2.py3-none-any.whl size=6225 sha256=3b8b845fc097304684623a41b1b96b3448b91560019d8c7a2119efce1458fe86\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/fd/e8/fb1a226f707c7e20dbed4c43f81b819d279ffd3b0e2f06ee13\n",
            "Successfully built pronouncing\n",
            "Installing collected packages: cmudict, pronouncing\n",
            "Successfully installed cmudict-0.4.5 pronouncing-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL70UxFeFxkZ"
      },
      "source": [
        "3. Run this chunk of code third to import all of the different libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKOo5txh1qb",
        "outputId": "0a041a13-430e-41d8-966e-7026f53490a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Imports\n",
        "import markovify\n",
        "import re\n",
        "import pronouncing\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YbGLxf0F87b"
      },
      "source": [
        "4. Run this chunk of code fourth - you can play around with the different values here, but don't make the depth too big, unless you have a lot of time to watch the network train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GrMPaYCkGfS"
      },
      "source": [
        "# These are just the parameters of the network.\n",
        "depth = 4  # depth of the network. changing will require a retrain\n",
        "maxsyllables = 16  # maximum syllables per line. Change this freely without retraining the network\n",
        "rap_length = 50 # number of lines in the rap song\n",
        "epochs_to_train = 3 # how many times the network trains on the whole dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHZnxNaaGGpl"
      },
      "source": [
        "This huge chunk of code just defines all of the functions used. Run it and scroll all the way down to generate a rap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4q26wKyg-Vy"
      },
      "source": [
        "def create_network(depth):\n",
        "    # Sequential() creates a linear stack of layers\n",
        "    model = Sequential()\n",
        "    # Adds a LSTM layer as the first layer in the network with\n",
        "    # 4 units (nodes), and a 2x2 tensor (which is the same shape as the\n",
        "    # training data)\n",
        "    model.add(LSTM(4, input_shape=(2, 2), return_sequences=True))\n",
        "    # adds 'depth' number of layers to the network with 8 nodes each\n",
        "    for i in range(depth):\n",
        "        model.add(LSTM(8, return_sequences=True))\n",
        "    # adds a final layer with 2 nodes for the output\n",
        "    model.add(LSTM(2, return_sequences=True))\n",
        "    # prints a summary representation of the model\n",
        "    model.summary()\n",
        "    # configures the learning process for the network / model\n",
        "    # the optimizer function rmsprop: optimizes the gradient descent\n",
        "    # the loss function: mse: will use the \"mean_squared_error when trying to improve\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='mse')\n",
        "\n",
        "    #if artist + \".rap\" in os.listdir(\".\") and train_mode == False:\n",
        "    #    # loads the weights from the hdf5 file saved earlier\n",
        "    #    model.load_weights(str(artist + \".rap\"))\n",
        "    #    print \"loading saved network: \" + str(artist) + \".rap\"\n",
        "    return model\n",
        "\n",
        "\n",
        "def markov(text_file):\n",
        "    read = rap_source\n",
        "    # markovify goes line by line of the lyrics.txt file and\n",
        "    # creates a model of the text which allows us to use\n",
        "    # make_sentence() later on to create a bar for lyrics\n",
        "    # creates a probability distribution for all the words\n",
        "    # so it can generate words based on the current word we're on\n",
        "    text_model = markovify.NewlineText(read)\n",
        "    return text_model\n",
        "\n",
        "\n",
        "# used when generating bars and making sure the length is not longer\n",
        "# than the max syllables, and will continue to generate bars until\n",
        "# the amount of syllables is less than the max syllables\n",
        "def syllables(line):\n",
        "    count = 0\n",
        "    for word in line.split(\" \"):\n",
        "        vowels = 'aeiouy'\n",
        "        word = word.lower().strip(\".:;?!\")\n",
        "        if word[0] in vowels:\n",
        "            count += 1\n",
        "        for index in range(1, len(word)):\n",
        "            if word[index] in vowels and word[index - 1] not in vowels:\n",
        "                count += 1\n",
        "        if word.endswith('e'):\n",
        "            count -= 1\n",
        "        if word.endswith('le'):\n",
        "            count += 1\n",
        "        if count == 0:\n",
        "            count += 1\n",
        "    return count / maxsyllables\n",
        "\n",
        "\n",
        "# writes a rhyme list to a rhymes file that allows for use when\n",
        "# building the dataset, and composing the rap\n",
        "def rhymeindex(lyrics):\n",
        "    #if str(artist) + \".rhymes\" in os.listdir(\".\") and train_mode == False:\n",
        "    #    print \"loading saved rhymes from \" + str(artist) + \".rhymes\"\n",
        "    #    return open(str(artist) + \".rhymes\", \"r\").read().split(\"\\n\")\n",
        "    if True:\n",
        "        rhyme_master_list = []\n",
        "        print \"Alright, building the list of all the rhymes\"\n",
        "        for i in lyrics:\n",
        "            # grabs the last word in each bar\n",
        "            word = re.sub(r\"\\W+\", '', i.split(\" \")[-1]).lower()\n",
        "            # pronouncing.rhymes gives us a word that rhymes with the word being passed in\n",
        "            rhymeslist = pronouncing.rhymes(word)\n",
        "            # need to convert the unicode rhyme words to UTF8\n",
        "            rhymeslist = [x.encode('UTF8') for x in rhymeslist]\n",
        "            # rhymeslistends contains the last two characters for each word\n",
        "            # that could potentially rhyme with our word\n",
        "            rhymeslistends = []\n",
        "            for i in rhymeslist:\n",
        "                rhymeslistends.append(i[-2:])\n",
        "            try:\n",
        "                # rhymescheme gets all the unique two letter endings and then\n",
        "                # finds the one that occurs the most\n",
        "                rhymescheme = max(set(rhymeslistends), key=rhymeslistends.count)\n",
        "            except Exception:\n",
        "                rhymescheme = word[-2:]\n",
        "            rhyme_master_list.append(rhymescheme)\n",
        "        # rhyme_master_list is a list of the two letters endings that appear\n",
        "        # the most in the rhyme list for the word\n",
        "        rhyme_master_list = list(set(rhyme_master_list))\n",
        "\n",
        "        reverselist = [x[::-1] for x in rhyme_master_list]\n",
        "        reverselist = sorted(reverselist)\n",
        "        # rhymelist is a list of the two letter endings (reversed)\n",
        "        # the reason the letters are reversed and sorted is so\n",
        "        # if the network messes up a little bit and doesn't return quite\n",
        "        # the right values, it can often lead to picking the rhyme ending next to the\n",
        "        # expected one in the list. But now the endings will be sorted and close together\n",
        "        # so if the network messes up, that's alright and as long as it's just close to the\n",
        "        # correct rhymes\n",
        "        rhymelist = [x[::-1] for x in reverselist]\n",
        "\n",
        "        #f = open(str(artist) + \".rhymes\", \"w\")\n",
        "        #f.write(\"\\n\".join(rhymelist))\n",
        "        #f.close()\n",
        "        print rhymelist\n",
        "        return rhymelist\n",
        "\n",
        "\n",
        "# converts the index of the most common rhyme ending\n",
        "# into a float\n",
        "def rhyme(line, rhyme_list):\n",
        "    word = re.sub(r\"\\W+\", '', line.split(\" \")[-1]).lower()\n",
        "    rhymeslist = pronouncing.rhymes(word)\n",
        "    rhymeslist = [x.encode('UTF8') for x in rhymeslist]\n",
        "    rhymeslistends = []\n",
        "    for i in rhymeslist:\n",
        "        rhymeslistends.append(i[-2:])\n",
        "    try:\n",
        "        rhymescheme = max(set(rhymeslistends), key=rhymeslistends.count)\n",
        "    except Exception:\n",
        "        rhymescheme = word[-2:]\n",
        "    try:\n",
        "        float_rhyme = rhyme_list.index(rhymescheme)\n",
        "        float_rhyme = float_rhyme / float(len(rhyme_list))\n",
        "        return float_rhyme\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# grabs each line of the lyrics file and puts them\n",
        "# in their own index of a list, and then removes any empty lines\n",
        "# from the lyrics file and returns the list as bars\n",
        "def split_lyrics_file(text):\n",
        "    #text = open(text_file).read()\n",
        "    #text = text.split(\"\\n\")\n",
        "    while \"\" in text:\n",
        "        text.remove(\"\")\n",
        "    return text\n",
        "\n",
        "\n",
        "# only ran when not training\n",
        "def generate_lyrics(lyrics_file):\n",
        "    bars = []\n",
        "    last_words = []\n",
        "    lyriclength = len(lyrics_file)\n",
        "    count = 0\n",
        "    markov_model = markov((\". \").join(lyrics_file) + \".\")\n",
        "\n",
        "    while len(bars) < lyriclength / 9 and count < lyriclength * 2:\n",
        "        # By default, the make_sentence method tries, a maximum of 10 times per invocation,\n",
        "        # to make a sentence that doesn't overlap too much with the original text.\n",
        "        # If it is successful, the method returns the sentence as a string.\n",
        "        # If not, it returns None. (https://github.com/jsvine/markovify)\n",
        "        bar = markov_model.make_sentence()\n",
        "\n",
        "        # make sure the bar isn't 'None' and that the amount of\n",
        "        # syllables is under the max syllables\n",
        "        if type(bar) != type(None) and syllables(bar) < 1:\n",
        "\n",
        "            # function to get the last word of the bar\n",
        "            def get_last_word(bar):\n",
        "                last_word = bar.split(\" \")[-1]\n",
        "                # if the last word is punctuation, get the word before it\n",
        "                if last_word[-1] in \"!.?,\":\n",
        "                    last_word = last_word[:-1]\n",
        "                return last_word\n",
        "\n",
        "            last_word = get_last_word(bar)\n",
        "            # only use the bar if it is unique and the last_word\n",
        "            # has only been seen less than 3 times\n",
        "            if bar not in bars and last_words.count(last_word) < 3:\n",
        "                bars.append(bar)\n",
        "                last_words.append(last_word)\n",
        "                count += 1\n",
        "\n",
        "    return bars\n",
        "\n",
        "\n",
        "# used to construct the 2x2 inputs for the LSTMs\n",
        "# the lyrics being passed in are lyrics (original lyrics if being trained,\n",
        "# or ours if it's already trained)\n",
        "def build_dataset(lyrics, rhyme_list):\n",
        "    dataset = []\n",
        "    line_list = []\n",
        "    # line_list becomes a list of the line from the lyrics, the syllables for that line (either 0 or 1 since\n",
        "    # syllables uses integer division by maxsyllables (16)), and then rhyme returns the most common word\n",
        "    # endings of the words that could rhyme with the last word of line\n",
        "    for line in lyrics:\n",
        "        line_list = [line, syllables(line), rhyme(line, rhyme_list)]\n",
        "        dataset.append(line_list)\n",
        "\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # using range(len(dataset)) - 3 because of the way the indices are accessed to\n",
        "    # get the lines\n",
        "    for i in range(len(dataset) - 3):\n",
        "        line1 = dataset[i][1:]\n",
        "        line2 = dataset[i + 1][1:]\n",
        "        line3 = dataset[i + 2][1:]\n",
        "        line4 = dataset[i + 3][1:]\n",
        "\n",
        "        # populate the training data\n",
        "        # grabs the syllables and rhyme index here\n",
        "        x = [line1[0], line1[1], line2[0], line2[1]]\n",
        "        x = np.array(x)\n",
        "        # the data is shaped as a 2x2 array where each row is a\n",
        "        # [syllable, rhyme_index] pair\n",
        "        x = x.reshape(2, 2)\n",
        "        x_data.append(x)\n",
        "\n",
        "        # populate the target data\n",
        "        y = [line3[0], line3[1], line4[0], line4[1]]\n",
        "        y = np.array(y)\n",
        "        y = y.reshape(2, 2)\n",
        "        y_data.append(y)\n",
        "\n",
        "    # returns the 2x2 arrays as datasets\n",
        "    x_data = np.array(x_data)\n",
        "    y_data = np.array(y_data)\n",
        "\n",
        "    # print \"x shape \" + str(x_data.shape)\n",
        "    # print \"y shape \" + str(y_data.shape)\n",
        "    return x_data, y_data\n",
        "\n",
        "# only used when not training\n",
        "def compose_rap(lines, rhyme_list, lyrics_file, model):\n",
        "    rap_vectors = []\n",
        "    human_lyrics = split_lyrics_file(lyrics_file)\n",
        "\n",
        "    # choose a random line to start in from given lyrics\n",
        "    initial_index = random.choice(range(len(human_lyrics) - 1))\n",
        "    # create an initial_lines list consisting of 2 lines\n",
        "    initial_lines = human_lyrics[initial_index:initial_index + 8]\n",
        "\n",
        "    starting_input = []\n",
        "    for line in initial_lines:\n",
        "        # appends a [syllable, rhyme_index] pair to starting_input\n",
        "        starting_input.append([syllables(line), rhyme(line, rhyme_list)])\n",
        "\n",
        "    # predict generates output predictions for the given samples\n",
        "    # it's reshaped as a (1, 2, 2) so that the model can predict each\n",
        "    # 2x2 matrix of [syllable, rhyme_index] pairs\n",
        "    starting_vectors = model.predict(np.array([starting_input]).flatten().reshape(4, 2, 2))\n",
        "    rap_vectors.append(starting_vectors)\n",
        "\n",
        "    for i in range(rap_length):\n",
        "        rap_vectors.append(model.predict(np.array([rap_vectors[-1]]).flatten().reshape(4, 2, 2)))\n",
        "\n",
        "    return rap_vectors\n",
        "\n",
        "\n",
        "def vectors_into_song(vectors, generated_lyrics, rhyme_list):\n",
        "    print \"\\n\\n\"\n",
        "    print \"About to write rap (this could take a moment)...\"\n",
        "    print \"\\n\\n\"\n",
        "\n",
        "    # compare the last words to see if they are the same, if they are\n",
        "    # increment a penalty variable which grants penalty points for being\n",
        "    # uncreative\n",
        "    def last_word_compare(rap, line2):\n",
        "        penalty = 0\n",
        "        for line1 in rap:\n",
        "            word1 = line1.split(\" \")[-1]\n",
        "            word2 = line2.split(\" \")[-1]\n",
        "\n",
        "            # remove any punctuation from the words\n",
        "            while word1[-1] in \"?!,. \":\n",
        "                word1 = word1[:-1]\n",
        "\n",
        "            while word2[-1] in \"?!,. \":\n",
        "                word2 = word2[:-1]\n",
        "\n",
        "            if word1 == word2:\n",
        "                penalty += 0.2\n",
        "\n",
        "        return penalty\n",
        "\n",
        "    # vector_half is a single [syllable, rhyme_index] pair\n",
        "    # returns a score rating for a given line\n",
        "    def calculate_score(vector_half, syllables, rhyme, penalty):\n",
        "        desired_syllables = vector_half[0]\n",
        "        desired_rhyme = vector_half[1]\n",
        "        # desired_syllables is the number of syllables we want\n",
        "        desired_syllables = desired_syllables * maxsyllables\n",
        "        # desired rhyme is the index of the rhyme we want\n",
        "        desired_rhyme = desired_rhyme * len(rhyme_list)\n",
        "\n",
        "        # generate a score by subtracting from 1 the sum of the difference between\n",
        "        # predicted syllables and generated syllables and the difference between\n",
        "        # the predicted rhyme and generated rhyme and then subtract the penalty\n",
        "        score = 1.0 - (abs((float(desired_syllables) - float(syllables))) + abs(\n",
        "            (float(desired_rhyme) - float(rhyme)))) - penalty\n",
        "\n",
        "        return score\n",
        "\n",
        "    # generated a list of all the lines from generated_lyrics with their\n",
        "    # line, syllables, and rhyme float value\n",
        "    dataset = []\n",
        "    for line in generated_lyrics:\n",
        "        line_list = [line, syllables(line), rhyme(line, rhyme_list)]\n",
        "        dataset.append(line_list)\n",
        "\n",
        "    rap = []\n",
        "\n",
        "    vector_halves = []\n",
        "    for vector in vectors:\n",
        "        # vectors are the 2x2 rap_vectors (predicted bars) generated by compose_rap()\n",
        "        # separate every vector into a half (essentially one bar) where each\n",
        "        # has a pair of [syllables, rhyme_index]\n",
        "        vector_halves.append(list(vector[0][0]))\n",
        "        vector_halves.append(list(vector[0][1]))\n",
        "\n",
        "    for vector in vector_halves:\n",
        "        # Each vector (predicted bars) is scored against every generated bar ('item' below)\n",
        "        # to find the generated bar that best matches (highest score) the vector predicted\n",
        "        # by the model. This bar is then added to the final rap and also removed from the\n",
        "        # generated lyrics (dataset) so that we don't get duplicate lines in the final rap.\n",
        "        scorelist = []\n",
        "        for item in dataset:\n",
        "            # item is one of the generated bars from the Markov model\n",
        "            line = item[0]\n",
        "\n",
        "            if len(rap) != 0:\n",
        "                penalty = last_word_compare(rap, line)\n",
        "            else:\n",
        "                penalty = 0\n",
        "            # calculate the score of the current line\n",
        "            total_score = calculate_score(vector, item[1], item[2], penalty)\n",
        "            score_entry = [line, total_score]\n",
        "            # add the score of the current line to a scorelist\n",
        "            scorelist.append(score_entry)\n",
        "\n",
        "        fixed_score_list = []\n",
        "        for score in scorelist:\n",
        "            fixed_score_list.append(float(score[1]))\n",
        "        # get the line with the max valued score from the fixed_score_list\n",
        "        max_score = max(fixed_score_list)\n",
        "        for item in scorelist:\n",
        "            if item[1] == max_score:\n",
        "                # append item[0] (the line) to the rap\n",
        "                rap.append(item[0])\n",
        "                print str(item[0])\n",
        "\n",
        "                # remove the line we added to the rap so\n",
        "                # it doesn't get chosen again\n",
        "                for i in dataset:\n",
        "                    if item[0] == i[0]:\n",
        "                        dataset.remove(i)\n",
        "                        break\n",
        "                break\n",
        "    return rap\n",
        "\n",
        "\n",
        "def train(x_data, y_data, model):\n",
        "    # fit is used to train the model for 5 'epochs' (iterations) where\n",
        "    # the x_data is the training data, and the y_data is the target data\n",
        "    # x is the training and y is the target data\n",
        "    # batch_size is a subset of the training data (2 in this case)\n",
        "    # verbose simply shows a progress bar\n",
        "    model.fit(np.array(x_data), np.array(y_data),\n",
        "              batch_size=4,\n",
        "              epochs=epochs_to_train,\n",
        "              verbose=1)\n",
        "    # save_weights saves the best weights from training to a hdf5 file\n",
        "    #model.save_weights(artist + \".rap\")\n",
        "\n",
        "\n",
        "def main(depth):\n",
        "    train_mode = True\n",
        "    model = create_network(depth)\n",
        "    # change the lyrics file to the file with the lyrics you want to be trained on\n",
        "    \n",
        "    text_file = rap_source\n",
        "    \n",
        "    bars = split_lyrics_file(text_file)\n",
        "\n",
        "\n",
        "\n",
        "    rhyme_list = rhymeindex(bars)\n",
        "    \n",
        "    x_data, y_data = build_dataset(bars, rhyme_list)\n",
        "    train(x_data, y_data, model)\n",
        "    \n",
        "    bars = generate_lyrics(text_file)\n",
        "    \n",
        "    vectors = compose_rap(bars, rhyme_list, text_file, model)\n",
        "    rap = vectors_into_song(vectors, bars, rhyme_list)\n",
        "    \n",
        "    for bar in rap:\n",
        "        print bar"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ln_WuWhryOj"
      },
      "source": [
        "Run this chunk of code below to call the main function and kick the rap generation off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXx1lJ9H-tPr",
        "outputId": "f7a53e18-ae02-4683-d8bb-d08991bccea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "main(depth) # run this to get the actual rap"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 2, 4)              112       \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 2, 8)              416       \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 2, 8)              544       \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 2, 8)              544       \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 2, 8)              544       \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 2, 2)              88        \n",
            "=================================================================\n",
            "Total params: 2,248\n",
            "Trainable params: 2,248\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Alright, building the list of all the rhymes\n",
            "['', \"s'\", '10', '30', '1', '11', '31', '12', '32', 'k2', '4', '5', '05', '25', '45', '55', '26', 'g6', '7', '8', '9', '99', 'a', 'aa', 'ba', 'ca', 'da', 'ga', 'ha', 'ia', 'ja', 'ka', 'la', 'ma', 'na', 'pa', 'ra', 'sa', 'ta', 'va', 'ya', 'za', 'ab', 'eb', 'ib', 'ob', 'ub', 'ac', 'fc', 'ic', 'nc', 'oc', 'ad', 'ed', 'id', 'ld', 'nd', 'od', 'rd', 'be', 'ce', 'de', 'ee', 'fe', 'ge', 'he', 'ie', 'ke', 'le', 'me', 'ne', 'oe', 'pe', 're', 'se', 'te', 've', 'ye', 'ze', 'af', 'ef', 'ff', 'if', 'lf', 'of', 'ag', 'eg', 'gg', 'ig', 'ng', 'og', 'rg', 'ug', 'ah', 'ch', 'gh', 'hh', 'oh', 'ph', 'sh', 'th', 'uh', 'ai', 'ci', 'gi', 'ii', 'ki', 'li', 'mi', 'ni', 'ri', 'ti', 'zi', 'pj', 'tj', 'ck', 'ek', 'lk', 'nk', 'ok', 'rk', 'sk', 'al', 'el', 'hl', 'il', 'll', 'ol', 'rl', 'tl', 'ul', 'xl', 'am', 'bm', 'em', 'im', 'lm', 'mm', 'om', 'rm', 'sm', 'um', 'an', 'en', 'gn', 'in', 'mn', 'nn', 'on', 'rn', 'un', 'wn', 'yn', 'ao', 'co', 'do', 'eo', 'fo', 'go', 'io', 'ko', 'lo', 'mo', 'no', 'oo', 'po', 'ro', 'to', 'vo', 'zo', 'ap', 'ep', 'ip', 'lp', 'mp', 'op', 'pp', 'rp', 'up', 'ar', 'cr', 'er', 'ir', 'jr', 'or', 'pr', 'rr', 'ur', \"'s\", '0s', 'as', 'bs', 'cs', 'ds', 'es', 'gs', 'hs', 'is', 'js', 'ks', 'ls', 'ms', 'ns', 'os', 'ps', 'rs', 'ss', 'ts', 'us', 'vs', 'ws', 'ys', \"'t\", 'at', 'ct', 'et', 'ft', 'ht', 'it', 'lt', 'nt', 'ot', 'pt', 'rt', 'st', 'tt', 'ut', 'au', 'cu', 'ru', 'uu', 'lv', 'ov', 'ew', 'lw', 'ow', 'ww', 'ax', 'ex', 'ix', 'ox', 'ay', 'by', 'cy', 'dy', 'ey', 'fy', 'gy', 'hy', 'ky', 'ly', 'my', 'ny', 'oy', 'ry', 'sy', 'ty', 'vy', 'zy', 'ez', 'iz', 'oz', 'tz', 'yz', 'zz']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}